{
  "agent_id": "coder2",
  "task_id": "task_4",
  "files": [
    {
      "name": "visualization.py",
      "purpose": "Results visualization",
      "priority": "medium"
    },
    {
      "name": "README.md",
      "purpose": "Project documentation",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_stat.ML_2508.21022v1_Fast_Convergence_Rates_for_Subsampled_Natural_Grad",
    "project_type": "optimization",
    "description": "Enhanced AI project based on stat.ML_2508.21022v1_Fast-Convergence-Rates-for-Subsampled-Natural-Grad with content analysis. Detected project type: optimization (confidence score: 7 matches).",
    "key_algorithms": [
      "Variational",
      "Newton",
      "Learning",
      "Both",
      "Analytical",
      "Second",
      "Appropriate",
      "Reconfiguration",
      "Machine",
      "Quadratic"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: stat.ML_2508.21022v1_Fast-Convergence-Rates-for-Subsampled-Natural-Grad.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nFAST CONVERGENCE RATES FOR SUBSAMPLED NAT-\nURAL GRADIENT ALGORITHMS ON QUADRATIC\nMODEL PROBLEMS\nGil Goldshlager\nDepartment of Mathematics\nUniversity of California, Berkeley\nggoldsh@berkeley.eduJiang Hu\nYau Mathematical Sciences Center\nTsinghua University\nhujiangopt@gmail.com\nLin Lin\nDepartment of Mathematics, University of California, Berkeley\nLawrence Berkeley National Laboratory\nlinlin@math.berkeley.edu\nABSTRACT\nSubsampled natural gradient descent (SNGD) has shown impressive results for\nparametric optimization tasks in scientific machine learning, such as neural net-\nwork wavefunctions and physics-informed neural networks, but it has lacked a\ntheoretical explanation. We address this gap by analyzing the convergence of\nSNGD and its accelerated variant, SPRING, for idealized parametric optimization\nproblems where the model is linear and the loss function is strongly convex and\nquadratic. In the special case of a least-squares loss, namely the standard linear\nleast-squares problem, we prove that SNGD is equivalent to a regularized Kacz-\nmarz method while SPRING is equivalent to an accelerated regularized Kaczmarz\nmethod. As a result, by leveraging existing analyses we obtain under mild condi-\ntions (i) the first fast convergence rate for SNGD, (ii) the first convergence guaran-\ntee for SPRING in any setting, and (iii) the first proof that SPRING can accelerate\nSNGD. In the case of a general strongly convex quadratic loss, we extend the\nanalysis of the regularized Kaczmarz method to obtain a fast convergence rate for\nSNGD under stronger conditions, providing the first explanation for the effective-\nness of SNGD outside of the least-squares setting. Overall, our results illustrate\nhow tools from randomized linear algebra can shed new light on the interplay\nbetween subsampling and curvature-aware optimization strategies.\n1 I NTRODUCTION\nNeural networks are gaining traction as an alternative to traditional numerical methods for solving\nscientific simulation problems. For example, in the quantum sciences, neural network wavefunctions\n(NNWs) have been used to represent quantum wavefunctions and find solutions to Schr \u00a8odinger\u2019s\nequation for model systems (Carleo & Troyer, 2017) as well as molecules and materials (Pfau et al.,\n2020; Hermann et al., 2020; Li et al., 2022). In parallel, neural networks have been applied to solve\nother, non-quantum partial differential equations, with such approaches collectively referred to as\nphysics-informed neural networks (PINNs, Raissi et al., 2019).\nDespite some initial successes, such scientific machine learning approaches face severe optimization\nchallenges arising from ill-conditioned and non-convex loss landscapes (Bukov et al., 2021; Krish-\nnapriyan et al., 2021; Rathore et al., 2024). One promising approach to address these challenges is\nto use optimization methods based on natural gradient descent (NGD, Amari, 1998), which takes\nadvantage of the geometry of the function space to accelerate convergence. For example, NNWs\nare most commonly trained with the stochastic reconfiguration algorithm (Sorella, 2001), which is\nsimply the quantum version of natural gradient descent (Pfau et al., 2020). In the PINNs community,\n1arXiv:2508.21022v1  [cs.LG]  28 Aug 2025\n\n--- Page 2 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nnatural gradient methods are less common but have been shown to outperform more commonly used\noptimizers (M \u00a8uller & Zeinhofer, 2023; Dangel et al., 2024).\nRecently, both fields have seen the introduction of subsampled natural gradient algorithms that\nscale to millions of parameters without approximation (Chen & Heyl, 2024; Guzm \u00b4an-Cordero et al.,\n2025). These methods take advantage of a low rank structure that arises when the network is trained\nusing mini-batches whose size is smaller than the number of parameters. In fact, such an approach\nwas proposed years earlier by (Ren & Goldfarb, 2019), but found limited success in traditional ma-\nchine learning applications. We refer to the basic version of this method as subsampled natural\ngradient descent, or SNGD. Even more recently, a variant of SNGD has been proposed which can\nleverage momentum to accelerate the convergence at no extra cost (Goldshlager et al., 2024). This\nmethod is known as subsampled projected-increment natural gradient descent, or SPRING for short.\nSince their introduction, the SNGD and SPRING algorithms have been applied and extended by a\nvariety of follow-up studies (Rende et al., 2024; Smith et al., 2024; Scherbela et al., 2025; Li et al.,\n2022; Gu et al., 2025), but a theoretical understanding of these algorithms has been lacking. In\nfact, most previous analyses of natural gradient methods have avoided the subsampled regime by\nassuming that either the gradient, the Fisher matrix, or both are available deterministically Amari\n(1998); Zhang et al. (2019); Martens (2020); Ren & Goldfarb (2019); Wu et al. (2024). In the rare\ncase that any work has considered a stochastic gradient andFisher matrix, the analysis has either\nyielded a slow convergence rate or none all Cai et al. (2019); Yang et al. (2020). As a result, the\nfollowing question has remained unanswered:\nHow can we explain the fast convergence of subsampled natural gradient algorithms for\nscientific machine learning tasks, namely their superiority relative to stochastic gradient\nmethods and the additional acceleration provided by SPRING?\nWe provide the first satisfying answers to this question by analyzing SNGD and SPRING for two\nidealized parametric optimization problems. The basis for both model problems is the general para-\nmetric optimization problem\nmin\n\u03b8L(v\u03b8) (1)\nwhere \u03b8\u2208Rnare the parameters to be optimized, v\u03b8\u2208 H is a parameterized function in some\nHilbert space H, andL:H \u2192 Ris a loss function defined on the Hilbert space. This form is\ngeneral enough to include both NNWs and PINNs as special cases.\nTo facilitate the analysis, we consider the scenario in which the model v\u03b8=J\u03b8is linear and the loss\nL(v)is a convex quadratic. We also assume for simplicity that the Hilbert space is H=Rmand the\nJacobian Jis of full column rank. These assumptions lead us to the following model problem:\nProblem 1 (Linear least-quadratics (LLQ)) .Minimize\nLLQ(\u03b8) =L(J\u03b8), (2)\nwhere J\u2208Rm\u00d7nandL(v) =1\n2v\u22a4Hv+v\u22a4q+cis a strongly convex quadratic function.\nWe call this problem linear least-quadratics since it is similar to linear least-squares except that\nthe squared error function has been replaced by a general strongly convex quadratic function. LLQ\nserves as a natural model problem for natural gradient optimization and can also arise from making a\nGauss-Newton approximation to the general parametric optimization problem eq. (1). We note that\nwhile LLQ is formally equivalent to minimizing a general quadratic function of \u03b8, its compositional\nstructure Q(J\u03b8)is important since it enables us to define non-trivial natural gradient algorithms\nwhich can leverage the availability of Jto accelerate convergence.\nIn the special case that L(v) =1\n2\u2225v\u2212b\u22252is a squared error function, we recover the standard linear\nleast squares, which we take as our second model problem:\nProblem 2 (Linear least-squares (LLS)) .Minimize\nLLS(\u03b8) =L(J\u03b8) (3)\nwhere J\u2208Rm\u00d7nandL(v) =1\n2\u2225v\u2212b\u22252is a squared error function.\n2\n\n--- Page 3 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nWe use linear least-squares as a starting point to inform our analysis of SNGD and SPRING, and\nits simple structure enables us to prove stronger results than are possible for the more general case\nof LLQ. Linear least-squares is also of independent interest as a model problem since it arises as a\nGauss-Newton approximation to the general nonlinear least-squares problem min\u03b81\n2\u2225v\u03b8\u2212b\u22252, which\nencompasses many machine learning tasks including PINNs.\nTo further facilitate the analysis, we focus on instances of Problems 1 and 2 which are consistent\nin that the optimal parameters \u03b8\u2217satisfy v\u2217\n\u03b8= arg minvL(v). This consistency condition is par-\nticularly relevant for scientific machine learning since in scientific applications it is often the case\nthat physically meaningful solutions can only be attained when the model v\u03b8is powerful enough to\nnearly exactly minimize L. It is also similar to the concept of interpolation which has previously\nbeen used to study the convergence of both first- and second-order stochastic optimization methods\nMa et al. (2018); Meng et al. (2020), and in the case of linear least-squares it simply means that the\nunderlying linear system is consistent, J\u03b8\u2217=b.\nWe now state our contributions:\n1.For consistent linear least-squares, we prove fast convergence rates for both SNGD\nand SPRING under mild conditions; see Theorem 1 . These results explain why sub-\nsampled natural gradient algorithms can outperform stochastic gradient descent. They\nalso represent the first convergence proof for SPRING in any setting and the first rigorous\ndemonstration that SPRING can accelerate SNGD, achieving up to a square-root improve-\nment in the convergence rate. The rates follow from new equivalences between SNGD and\na regularized Kaczmarz method Goldshlager et al. (2025) and between SPRING and an\naccelerated regularized Kaczmarz method (ARK, Derezi \u00b4nski et al., 2025). The connection\nto ARK additionally provides the first principled explanation for the structure of SPRING.\n2.For consistent linear least-quadratics, we prove a fast convergence rate for SNGD\nunder stronger conditions; see Theorem 4 and Corollary 5 . This result provides the first\nexplanation for why subsampled natural gradient algorithms can be effective for problems\nwithout a least-squares structure, such as when training NNWs. The proof generalizes\nthe analysis of the regularized Kaczmarz method to incorporate the effects of the function\nspace hessian H. We supplement our analysis with numerical experiments demonstrating\nthat SPRING can dramatically outperform SNGD for LLQ, making the analysis of SPRING\nfor LLQ a tantalizing direction for future research; see Figure 1.\nOverall, our work demonstrates the power of viewing subsampled natural gradient algorithms\nthrough the lens of randomized linear algebra, using this perspective to provide the first mean-\ningful bounds on their convergence rates. Our approach may also have broader implications for\nthe analysis of subsampled curvature-aware optimization strategies such as subsampled Newton or\nGauss-Newton algorithms.\n1.1 R ELATED WORKS\nIn the context of PINNs, one recent work has analyzed the convergence properties of NGD in the\ndeterministic setting Xu et al. (2024) and another has analyzed a damped Newton method Rathore\net al. (2024). In the context of NNWs, two previous works have analyzed the convergence properties\nof stochastic gradient descent Li et al. (2023); Abrahamsen et al. (2024). Additionally, it has been\nproposed to apply acceleration techinques directly in the function space to further accelerate the\nconvergence of SPRING Li et al. (2025), and it has also been proposed to use Gauss-Newton or\nRiemannian Newton methods Webber & Lindsey (2022); Armegioiu et al. (2025). In a more general\nmachine learning context, one previous work analyzed a scheme that is equivalent to our SNGD\nCai et al. (2019), but attained much weaker bounds on the convergence rate and only considered\nthe least-squares setting. Finally, the previous works Goldshlager et al. (2024; 2025) have also\nmade connections between subsampled natural gradient algorithms and randomized block Kaczmarz\nmethods, but did not identify direct equivalences between the two and did not use these connections\nto obtain convergence guarantees for either SNGD or SPRING.\nSubsampling has also been studied in the context of optimization methods other than natural gradi-\nent descent. For example, subsampled proximal point algorithms have been proposed and analyzed\nfor solving stochastic optimization problems with objective functions of the least-squares type Bert-\n3\n\n--- Page 4 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nsekas (2011); Asi & Duchi (2019); Davis & Drusvyatskiy (2019). Subsampled Newton methods\nhave also been investigated in Roosta-Khorasani & Mahoney (2016a;b); Bottou et al. (2018); Bol-\nlapragada et al. (2019), where two separate mini-batches are used to construct the gradient and\nHessian approximations to ensure fast convergence.\n2 B ACKGROUND\n2.1 N EURAL NETWORK WAVEFUNCTIONS\nOur first motivating application is the use of neural networks to find ground state wavefunctions for\nquantum systems Carleo & Troyer (2017). In this setting, the wavefunction is parameterized by \u03a8\u03b8\nand ground states are found by minimizing the variational Monte Carlo loss\nLVMC(\u03b8) =\u27e8\u03a8\u03b8, H\u03a8\u03b8\u27e9\n\u27e8\u03a8\u03b8,\u03a8\u03b8\u27e9, (4)\nwhere His the Hamiltonian for the system of interest. To apply natural gradient methods, the para-\nmetric model is defined to be the normalized wavefunction v\u03b8= \u03a8 \u03b8/\u2225\u03a8\u03b8\u2225and the corresponding\nfunction space loss is LVMC(v) =\u27e8v, Hv\u27e9. Clearly LVMC does not exhibit a least-squares structure,\nmaking LLQ the appropriate model problem for training NNWs.\n2.2 P HYSICS -INFORMED NEURAL NETWORKS\nOur second motivating application is the use of neural networks to solve boundary value problems\n{Du=fon\u2126,u=gon\u2202\u2126}where \u2126is a physical domain with boundary \u2202\u2126andDis a differ-\nential operator. PINNs proceed by parameterizing u:=u\u03b8by a neural network and minimizing\nLPINN(\u03b8) =1\n2Z\n\u2126(Du\u03b8(s)\u2212f(s))2ds+w\n2Z\n\u2202\u2126(u\u03b8(s)\u2212g(s))2ds, (5)\nwhere wis a hyperparameter that controls the relative importance of the two loss terms. There are\nmultiple ways to split the PINN loss eq. (5) to obtain a parametric optimization problem, but we\nfocus on the split used by energy natural gradient descent (ENGD, M \u00a8uller & Zeinhofer, 2023) since\nthis is the setting in which SNGD and SPRING have been demonstrated to be effective Guzm \u00b4an-\nCordero et al. (2025). This formulation can be summarized as LPINN(\u03b8) =1\n2\u2225v\u03b8\u2212b\u22252where\nv\u03b8=\u0014\nDu\u03b8\u221aw\u00b7u\u03b8\u0015\n, b =\u0014\nf\u221aw\u00b7g\u0015\n. (6)\nThis nonlinear least-squares form motivates the use of LLS as a model for PINNs.\n2.3 N ATURAL GRADIENT DESCENT\nFor the general parametric optimization problem eq. (1) natural gradient descent is defined by the\niteration\n\u03b8t+1=\u03b8t\u2212\u03b7(J\u22a4J+\u03bbI)\u22121\u2207\u03b8L(v\u03b8t), (7)\nwhere J=J\u03b8v\u03b8tis the model Jacobian ,\u03bbis a regularization parameter, and \u03b7is a step size. The\nmatrix J\u22a4Jis known as the Fisher information matrix and encodes the geometry of the Hilbert space\nH, potentially accelerating the convergence relative to gradient descent. Unfortunately, directly\nevaluating eq. (7) has a cost of O(n3)due to the matrix inversion, which is far too expensive for\nmodern machine learning applications for which ncan be in the millions.\n2.4 S UBSAMPLED NATURAL GRADIENT ALGORITHMS\nUsing the chain rule, we can decompose \u2207\u03b8L(v\u03b8t) =J\u22a4rwhere r=\u2207vL(v\u03b8t)\u2208 H is the function\nspace gradient , or equivalently the dual of the Fr \u00b4echet derivative of Levaluated at v\u03b8t. We can then\nwrite NGD in the form\n\u03b8t+1=\u03b8t\u2212\u03b7(J\u22a4J+\u03bbI)\u22121J\u22a4r:=\u03b8t\u2212\u03b7J+(\u03bb)r, (8)\nwhere in the last step we have defined a compressed notation for the regularized pseudoinverse of J.\n4\n\n--- Page 5 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nAlgorithm 1 General Subsampled Natural Gradient Algorithm for Parametric Optimization\nInput: Parametric model v\u03b8, loss function L\nInput: Sample size k, sampling distribution \u03c1, initial guess \u03b80, total iterations T\nInput: Step size \u03b7, regularization \u03bb, momentum \u00b5{\u00b5= 0\u2192SNGD, \u00b5\u2208(0,1)\u2192SPRING }\n\u03d50\u21900\nfort= 0toT\u22121do\nSample S\u223c\u03c1\nJS\u2190 \u2207 \u03b8((v\u03b8t)S){Using backpropagation }\nrS\u2190(\u2207vL(v\u03b8))S{For example by applying differential operators to the model }\n\u03d5t+1\u2190\u00b5\u03d5t+J\u22a4\nS(JSJ\u22a4\nS+\u03bbI)\u22121(rS\u2212\u00b5JS\u03d5t)\n\u03b8t+1\u2190\u03b8t\u2212\u03b7\u03d5t+1\nend for\nReturn \u03b8T\nIn practice, the values of Jandrare never available over the entire physical domain at once, but\nare rather sampled at a small number of points for each iteration. Supposing a mini-batch size\nofkand denoting the sampled points collectively by S, the quantities that we have available in\npractice are thus JS\u2208Rk\u00d7nandrS\u2208Rk. Indeed, for both NNWs and PINNs these quantities are\nroutinely computed for finite sample sets Sby using automatic differentiation libraries to both apply\ndifferential operators and take derivatives with respect to the parameters. For the purposes of our\nanalysis, we assume that Sis always sampled uniformly without replacement from {1, . . . , m }.\nIn the common case that k\u226an, significant computational savings can be had by applying the\nWoodbury matrix identity to eq. (8):\nJ+(\u03bb)\nS = (J\u22a4\nSJS+\u03bbI)\u22121J\u22a4\nS=J\u22a4\nS(JSJ\u22a4\nS+\u03bbI)\u22121. (9)\nUsing the final expression the matrix to be inverted now has size k\u00d7kand the bottleneck step\nbecomes the formation of JSJ\u22a4\nSwhich has a cost of O(nk2): an enormous reduction relative to\nna\u00a8\u0131ve NGD. The resulting method is known as subsampled natural gradient descent (SNGD) and\nhas been proposed, for example by (Ren & Goldfarb, 2019; Chen & Heyl, 2024).\nTo accelerate the convergence of SNGD, (Goldshlager et al., 2024) introduced the SPRING algo-\nrithm which is essentially a momentum method that is specially designed to be compatible with\nSNGD. In the SPRING algorithm, a momentum vector \u03d5tis tracked alongside the parameter vector\n\u03b8tand the two iterates are updated using the equations\n\u03d5t+1=\u00b5\u03d5t+J+(\u03bb)\nS(rS\u2212\u00b5JS\u03d5t),\n\u03b8t+1=\u03b8t\u2212\u03b7\u03d5t+1,(10)\nwhere \u00b5\u2208(0,1)is a momentum coefficient typically taken to be slightly less than 1. The SPRING\nalgorithm was originally motivated by intuitive connections to the randomized block Kaczmarz\nmethod, but no rigorous connection was established and no convergence guarantees have been pro-\nvided until this point. To summarize, in Algorithm 1 we present unified pseudocode for the SNGD\nand SPRING algorithms for general parametric optimization problems.\n3 A NALYSIS OF SNGD AND SPRING FOR LINEAR LEAST -SQUARES\nThe linear least-squares problem is defined by v\u03b8=J\u03b8andL(v) =1\n2\u2225v\u2212b\u22252, and it thus follows\nthat the model Jacobian is \u2207\u03b8v\u03b8=Jand the function space gradient is \u2207vL(v\u03b8) =J\u03b8\u2212b. As a\nresult, the SNGD algorithm takes the form\n\u03b8t+1=\u03b8t\u2212\u03b7J+(\u03bb)\nS(JS\u03b8t\u2212bS). (11)\nNearly all previous analyses of SNGD have viewed this iteration through lens of stochastic opti-\nmization theory, splitting the update direction as\nJ+(\u03bb)(JS\u03b8t\u2212bS) = [( JSJ\u22a4\nS+\u03bbI)\u22121][J\u22a4\nS(Js\u03b8\u2212bs)] (12)\nand then attempting to analyze the properties of the stochastic preconditioner (JSJ\u22a4\nS+\u03bbI)\u22121and\nthe stochastic gradient J\u22a4\nS(Js\u03b8\u2212bs). This approach has the major limitation that for small samples\n5\n\n--- Page 6 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nsizes, little can be said about the inverse of the subsampled preconditioner. It also makes it difficult to\nhandle the correlation between the two stochastic estimators, which has led most previous analyses\nto assume separate samples S, S\u2032for each piece.\nWe resolve these challenges by instead viewing SNGD as a randomized block Kaczmarz method\nNeedell & Tropp (2014). Following previous analyses of such methods, we invoke the consistency\nassumption J\u03b8\u2217=bto rewrite the SNGD iteration eq. (11) as\n\u03b8t+1=\u03b8t\u2212\u03b7(J+(\u03bb)\nSJS)(\u03b8t\u2212\u03b8\u2217). (13)\nAll the stochasticity is now bundled into the term J+(\u03bb)\nSJSwhich is a regularized projector onto the\nrow space of JS. Denoting this regularized projector by P(S)and subtracting \u03b8\u2217from both sides of\neq. (13), we can write the SNGD iteration in the form\n(\u03b8t+1\u2212\u03b8\u2217) = (I\u2212\u03b7P(S))(\u03b8t\u2212\u03b8\u2217). (14)\nIt is now apparent that rather than complicating things, the use of the same minibatch Sfor both\nthe stochastic gradient and the stochastic preconditioner is favorable and results in a consistent\nreduction of the solution error . Indeed analyzing P(S)as a whole is much more tractable than an-\nalyzing the inverse of the stochastic preconditioner alone, and many previous works on randomized\nKaczmarz and related methods have analyzed the properties of such projection matrices Needell &\nTropp (2014); Gower & Richt \u00b4arik (2015); Derezi \u00b4nski & Rebrova (2024); Derezi \u00b4nski et al. (2025).\nBuilding on this insight we obtain the following results:\nTheorem 1 (Equivalences and convergence rates for linear least-squares) .For linear least-squares,\nthe SNGD algorithm with \u03b7= 1is directly equivalent to the regularized Kaczmarz method of Gold-\nshlager et al. (2025). Moreover, The SPRING algorithm is equivalent to the Nesterov-accelerated\nregularized Kaczmarz (ARK) method of Derezi \u00b4nski et al. (2025) under an appropriate transforma-\ntion of the iterates and hyperparameters.\nAs a result, for consistent problems J\u03b8\u2217=b, the SNGD algorithm with \u03b7= 1satisfies\nE\u2225\u03b8t\u2212\u03b8\u2217\u22252\u2264(1\u2212\u03b1)t\u2225\u03b80\u2212\u03b8\u2217\u22252(15)\nwhere P=ES[P(S)]and\u03b1=\u03bbmin(P). Furthermore, for appropriate choices of \u03b7and\u00b5SPRING\nsatisfies\nE\u2225\u03b8t\u2212\u03b8\u2217\u22252=O\u0010\n(1\u2212p\n\u03b1/\u03b2)t\u0011\n\u2225\u03b80\u2212\u03b8\u2217\u22252(16)\nwhere \u03b2=\u03bbmax\u0010\nESh\n(P\u22121/2P(S)P\u22121/2)2i\u0011\n.\nAs discussed in (Goldshlager et al., 2025, Corollary 3.3), the convergence rate (1\u2212\u03b1)tfor SNGD\ncan be much faster than the convergence rate of mini-batch stochastic gradient descent (SGD) for the\nsame problem, which helps to explain the fast convergence of SNGD for PINNs Guzm \u00b4an-Cordero\net al. (2025). In particular, the rate of SGD with a mini-batch size of kcan be at best (1\u2212k\u03ba\u22122\ndem(J))t\nwhere \u03badem(J) =\u2225J\u2225F\u2225J+\u22252is the Demmel condition number of J. On the other hand, the\nparameter \u03b1in SNGD can be significantly larger than k\u03ba\u22122\ndem(J). For example, when the rows of\nJare generated from a Gaussian distribution and the eigenvalues values of J\u22a4Jdecay as k\u2212\u03b2, it\nholds in the limit m\u2192 \u221e , \u03bb\u21920that\u03b1= \u2126( k\u03b2\u03ba\u22122\ndem(J)). The assumption of Gaussian data\nis idealized and can be relaxed, for example to handle sub-Gaussian distributions or other types\nof \u201cincoherent\u201d data Derezi \u00b4nski & Rebrova (2024). Furthermore, the assumption of singular value\ndecay is realistic for PINNs Wang et al. (2022). The advantage of SNGD over SGD can even extend\nto inconsistent problems, especially when tail averaging is applied to the iterates as in Epperly et al.\n(2024); Goldshlager et al. (2024). In summary:\nSNGD converges rapidly for linear least-squares due to its equivalence with the regularized\nKaczmarz method.\nAdditionally, it can be easily verified using P\u22121\u2aaf1\n\u03b1Ithat1\u2264\u03b2\u22641\n\u03b1, which implies that the\nconvergence rate of SPRING in Theorem 1 entails up to a square-root speed up relative to the con-\nvergence rate of SNGD. This helps to explain why SPRING can accelerate SNGD for PINNs and\n6\n\n--- Page 7 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nNatural Gradient Algorithm Least-Squares Solver\nSNGD Regularized Kaczmarz, Goldshlager et al. (2025)\nSPRING Accelerated Regularized Kaczmarz, Derezi \u00b4nski et al. (2025)\nTable 1: Equivalences between algorithms in the linear least-squares setting.\nprovides the first convergence proof for SPRING of any kind. It also explains the empirical obser-\nvation that the benefits of SPRING are greater for small batch sizes Guzm \u00b4an-Cordero et al. (2025),\nsince for smaller batch sizes \u03b1is smaller and thus the gap between the (1\u2212\u03b1)tconvergence rate of\nSNGD and the best-case (1\u2212\u221a\u03b1)tconvergence rate of SPRING is more significant. Going beyond\nthe convergence rates, the equivalence between SPRING and ARK suggests that the previously ad\nhoc structure of the SPRING algorithm arises naturally from viewing it as an accelerated regularized\nKaczmarz method. Conversely, using our equivalence proof to translate the ARK method into the\nform of SPRING yields a new way to formulate ARK which is more concise than previous formula-\ntions such as those of Gower et al. (2018); Derezi \u00b4nski et al. (2025). Lastly, the equivalence between\nSPRING and ARK suggests that the adaptive momentum technique from (Derezi \u00b4nski et al., 2025)\nmay be applicable to SPRING as well, potentially paving a path towards a more black-box version\nof the SPRING optimizer. In summary:\nSPRING outperforms SNGD for consistent linear least-squares due to its equivalence with\nthe Nesterov-accelerated regularized Kaczmarz method.\nWe catalog the equivalences between natural gradient algorithms and least-squares solvers in Ta-\nble 1, and the proof Theorem 1 can be found in Appendix A.\n4 A NALYSIS OF SNGD FOR LINEAR LEAST -QUADRATICS\nWe now turn our attention to the linear least-quadratics Problem 1 which serves as a model for more\ngeneral parametric optimization problems such as NNWs for which the loss does not exhibit a least-\nsquares structure. LLQ is defined by v\u03b8=J\u03b8andL(v) =1\n2v\u22a4Hv+v\u22a4q+cand it thus follows\nthat the model Jacobian is \u2207\u03b8v\u03b8=Jand the function space gradient is \u2207vL(v\u03b8) =HJ\u03b8 +q. As a\nresult, the SNGD algorithm takes the form\n\u03b8t+1=\u03b8t\u2212\u03b7J+(\u03bb)\nS(HSJ\u03b8t+qS). (17)\nWe now show that just as in the case of linear least-squares, we can leverage an appropriate con-\nsistency assumption to translate this iteration into a more amenable form where the stochasticity is\nbundled into the regularized projector P(S). We note that unlike the previous analysis which was\ndrawn from the randomized block Kaczmarz literature, the following analysis is all new.\nOur starting point is the observation that the SNGD iteration eq. (17) is equivalent to a regular-\nized Kaczmarz iteration for the least-squares subproblem min\u03b8\u2225J\u03b8\u2212(HJ\u03b8 +q)\u22252. To obtain the\nbenefits of the regularized Kaczmarz method, it is thus natural to assume that this subproblem is\nconsistent, namely that HJ\u03b8 +q\u2208range( J). The intuitive meaning of this assumption is that the\nfunction space gradient always lies within the tangent space of the parametric model. In the setting\nof NNWs, this property could hold if the neural network is powerful enough to represent the entire\nHilbert space H, or more realistically if the wavefunction \u03a8\u03b8remains within a low-energy subspace\nthat can be fully represented by the neural network. To ensure that this containment property holds\nin the case of LLQ we introduce the following strong consistency condition:\nAssumption 2 (Strong consistency for linear least-quadratics) .An LLQ problem is strongly consis-\ntent if the following two conditions hold:\n1. The problem is consistent, namely J\u03b8\u2217= arg minvL(v).\n2.range( HJ)\u2286range( J).\n7\n\n--- Page 8 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nHere point (1) ensures that \u2207vL(v\u03b8\u2217) =HJ\u03b8\u2217+b= 0, which implies HJ\u03b8 +q=HJ(\u03b8\u2212\u03b8\u2217).\nPoint (2) then ensures that HJ(\u03b8\u2212\u03b8\u2217)\u2208range( J), which confirms that HJ\u03b8 +q\u2208range( J).\nNote that for the special case of linear least-squares, Assumption 2 is equivalent to the standard\nconsistency assumption since H=I=\u21d2range( HJ) = range( J).\nUnder Assumption 2 since HJ\u03b8 +q\u2208range( J)it follows that HJ\u03b8 +q=J+J(HJ\u03b8 +q)and so\nthe SNGD direction can be rewritten as\nJ+(\u03bb)\nS(HSJ\u03b8t+qS) =J+(\u03bb)\nS(HJ\u03b8 t+q)S\n=J+(\u03bb)\nS(JJ+(HJ\u03b8 t+q))S\n=P(S)J+(HJ\u03b8 t+q).\nPlugging this into eq. (17), we can rewrite the entire SNGD iteration as\n\u03b8t+1=\u03b8t\u2212\u03b7P(S)J+(HJ\u03b8 t+q). (18)\nAs promised, we have now bundled all of the stochasticity into the regularized projector P(S). The\nresult is that under an appropriate consistency condition, the SNGD update for LLQ arises\nfrom applying the regularized projector P(S)to the deterministic, unregularized natural\ngradient direction J+(HJ\u03b8 t+q). This insight is critical to our analysis of SNGD for LLQ\nand provides an intuitive explanation for why SNGD can enjoy some of the same benefits as NGD.\n4.1 C ONDITIONS FOR THE CONVERGENCE OF SNGD FOR LLQ\nEven with Assumption 2 and the resulting appealing eq. (18), it is not clear whether the combination\nof subsampling and the anisotropic function-space hessian Hcan lead to convergent algorithms. In\nfact, it turns out that SNGD can still fail to converge for LLQ when \u03bbis too small:\nProposition 3 (Conditions for the convergence of SNGD for linear least-quadratics) .Consider a\nfixed instance of Problem 1 for which Assumption 2 holds and let P=ES[P(S)]as in Theorem 1.\nDefine M=PJ+HJand let \u03be(\u03bb)be the minimum real part of any eigenvalue of M, as a function\nof\u03bb. Then:\n1. For fixed \u03bb, if\u03be(\u03bb)<0then there exists an initial guess \u03b80such that SNGD is divergent\nforevery choice of \u03b7 >0.\n2. Conversely, there exists \u03bb0such that for any \u03bb > \u03bb 0,\u03be(\u03bb)>0and SNGD is convergent\nfor a small enough choice of the step size \u03b7and any initial guess \u03b80.\nThe proof of Proposition 3 can be found in Appendix B and numerical examples manifesting the\nfailure condition \u03be(\u03bb)<0can be found in Appendix C. These results provide a new explanation\nfor the common observation that it is essential to choose \u03bb >0when training NNWs, beyond\nmaintaining the invertibility of J\u22a4\nSJSor reducing the variance of the iterates. It is worth noting\nthat without subsampling, the convergence of NGD for LLQ is less nuanced and confirms previous\nintuitions about NGD; see Proposition 8 in Appendix B for further discussion of this point.\n4.2 F AST CONVERGENCE OF SNGD FOR LLQ\nProposition 3 poses a hurdle to obtaining an explicit convergence rate for SNGD for LLQ, since\nit is difficult to determine how large \u03bbmust be to ensure convergence and when \u03bbis large, the\nconvergence can be slowed substantially. To facilitate the analysis we thus introduce the ad-\nditional assumption that Pshares an eigenbasis with J\u22a4J. This guarantees \u03be(\u03bb)>0since\nM=\u0000\nP(JTJ+\u03bbI)\u22121\u0001\u0000\nJTHJ\u0001\nbecomes a product of two positive definite factors. While this\nassumption is not expected to hold in practice, it is natural in that it holds for two special cases: (i)\nfor arbitrary Jit holds when sampling Sfrom an appropriate determinantal point process, and (ii)\nwhen the rows of Jarise from a Gaussian distribution and it holds asymptotically as m\u2192 \u221e ; see\nProposition 9 in Appendix B for details. Concretely we obtain the following main result:\nTheorem 4 (Convergence rate of SNGD for linear least-quadratics) .Suppose that Assumption 2\nholds and additionally Pshares an eigenbasis with J\u22a4J. Define the positive definite matrix Q=\n(J\u22a4J)\u22121/2P(J\u22a4J)\u22121/2and let\n\u03b3= 1/\u03bbmax\u0010\nEh\n(J\u22a4J)\u22121/2P(S)Q\u22121P(S)(J\u22a4J)\u22121/2i\u0011\n. (19)\n8\n\n--- Page 9 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nThen \u03b3\u2265\u03ba\u22121(Q)and with step size \u03b7=\u03b3/\u03bbmax(\u02dcH), the expected squared error of the SNGD\niterates for Problem 1 satisfies\nE\u2225\u03b8t\u2212\u03b8\u2217\u22252\nQ\u22121\u2264\u0010\n1\u2212\u03ba\u22121(\u02dcH)\u00b7\u03b1\u00b7\u03b3\u0011t\n\u2225\u03b80\u2212\u03b8\u2217\u22252\nQ\u22121. (20)\nThis result represents the first convergence proof for SNGD outside of a least-squares setting. Inter-\nestingly, relative to LLS the convergence rate of SNGD is slowed down by a factor of \u03b3in addition to\nthe expected factor of \u03ba\u22121(\u02dcH). The reason for this extra slow-down is that even with the alignment\nassumption regarding PandJ\u22a4J, the expected step matrix Mis still not symmetric. To address\nthis issue we measure the solution error in the norm defined by the inverse of Q, but this worsens\nthe quadratic term in the error bound. Nonetheless, the convergence rate of SNGD in eq. (20) can\nbe much faster than that of mini-batch SGD:\nCorollary 5 (Convergence rate comparison for linear least-quadratics) .Suppose that the conditions\nof Theorem 4 hold and additionally the rows of Jare generated from a Gaussian distribution whose\ncovariance matrix \u03a3exhibits polynomial spectral decay, i.e., \u03c3i(\u03a3)\u2264i\u2212\u03b2\u03c31(\u03a3)for\u03b2 >1, where\n\u03c3i(\u03a3)denotes the i-th largest singular value of \u03a3. Then in the limit of m\u2192 \u221e ,\u03bb\u21920and assuming\nk\u2264n/2, it holds \u03b1= \u2126(k\u03b2\u03ba\u22122\ndem(J)and\u03b3= \u2126(\u03ba2(J)). As a result, the SNGD algorithm satisfies\nE\u2225\u03b8t\u2212\u03b8\u2217\u2225Q\u22121\u2264\u0010\n1\u2212C\u03ba\u22121(\u02dcH)\u00b7k\u03b2\u03ba\u22122\ndem(J)\u00b7\u03ba\u22122(J)\u0011t\n\u2225\u03b80\u2212\u03b8\u2217\u22252\nQ\u22121 (21)\nfor a universal constant C > 0. In comparison, the best convergence rate we can attain for mini-\nbatch SGD is E\u2225\u03b8t\u2212\u03b8\u2217\u22252\u2264(1\u2212\u03b1SGD)t\u2225\u03b80\u2212\u03b8\u2217\u22252where\n\u03b1SGD\u2264\u02dcC\u03ba(\u02dcH)\u00b7k\u03ba\u22122\ndem\u0000\n(J\u22a4\u02dcHJ)1/2\u0001\n\u00b7\u03ba\u22121(J\u22a4\u02dcHJ) (22)\nfor\u02dcC=1\n2max i\u2225(\u02dcHJ)i\u2225\n\u2225\u02dcH\u22252\u2225Ji\u2225.\nThe dependence of the SNGD convergence rate on kandJis ask\u03b2\u03ba\u22122\ndem(J)\u03ba\u22122(J),whereas the\ndependence of the SGD convergence rate is at best k\u03ba\u22122\ndem(J)\u03ba\u22122(J)using \u03ba2\ndem((J\u22a4\u02dcHJ)1/2)\u2264\n\u03ba2\ndem(J)\u03ba(\u02dcH)and\u03ba(J\u22a4\u02dcHJ)\u2264\u03ba2(J)\u03ba(H). As such, whenkand\u03b2are even moderately large,\nSNGD can converge much more rapidly than SGD . We note as before that the assumption of\nGaussian data is idealized and can likely be relaxed, while rapid singular value decay is realistic\nfor NNWs Park & Kastoryano (2020). It is also worth noting that as in the case of SNGD, the\nconvergence rate of SGD for LLQ is slower than might be expected. For SGD, the reason for\nthe slow-down lies in the fact that the asymmetric stochastic gradient estimator J\u22a4\nSHSJdoes not\ncorrespond to the gradient of any convex component function. Consequently, the existing analysis of\nNeedell et al. (2014) which would yield a \u03ba\u22122\ndem(J)dependence is not applicable here. In summary:\nUnder appropriate conditions, the analysis of the regularized Kaczmarz method can be\ngeneralized to prove that SNGD converges rapidly for linear least-quadratics.\nThe proofs of Theorem 4 and Corollary 5 can be found in Appendix B.\n5 T OWARDS AN ANALYSIS OF SPRING FOR LINEAR LEAST -QUADRATICS\nThe analysis of SPRING for linear least-quadratics is more challenging due to the interaction be-\ntween the acceleration scheme and the function space hessian H. As such, we for now provide only\npreliminary explorations. We first note that SPRING can be analyzed easily in two extreme cases:\nProposition 6 (Convergence rate of SPRING for linear least-quadratics in extreme cases) .When\nk=mand\u03bb= 0, SPRING becomes equivalent to NGD and its convergence thus follows from\nProposition 8. Furthermore, when H=I, LLQ reduces to LLS and the convergence of SPRING\nthus follows from Theorem 1. In both of these cases, it holds\nE\u2225\u03b8t\u2212\u03b8\u2217\u22252=O\u0012\u0010\n1\u2212\u03ba\u22121(\u02dcH)p\n\u03b1/\u03b2\u0011t\u0013\n\u2225\u03b80\u2212\u03b8\u2217\u22252. (23)\n9\n\n--- Page 10 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nWhile this convergence rate is appealing, it seems unrealistic to hope that the factor of \u03b3from\nTheorem 4 will disappear when analyzing SPRING. We thus formulate the following conjecture:\nConjecture 7 (Convergence rate of SPRING for linear least-quadratics in general) .When Assump-\ntion 2 holds and additionally Pcommutes with J\u22a4J, and with an appropriate choice of hyperpa-\nrameters, SPRING for LLQ satisfies\nE\u2225\u03b8t\u2212\u03b8\u2217\u22252=O\u0010\n(1\u2212\u03ba(\u02dcH)\u22121p\n\u03b1/\u03b2\u00b7\u03b3)\u0011t\n\u2225\u03b80\u2212\u03b8\u2217\u22252. (24)\nWe defer the resolution of this conjecture to future work. In the meantime, we numerically test SGD,\nSNGD and SPRING for two difficult instances of Problem 1. The results in Figure 1 demonstrate\nthat SPRING dramatically outperforms both SGD and SNGD for these problems, providing support\nfor the spirit of our conjecture. We provide further details on these two experiments and an additional\nablation study regarding the batch size kin Appendix D. In summary:\nSPRING can dramatically outperform SNGD for linear least-quadratics, and future works\nshould aim to provide a theoretical explanation for this phenomenon.\n0 10000 20000 30000 40000 50000\nIteration1010\n108\n106\n104\n102\n100102Solution error ||*||\n(J)=104, (H)=101\nSGD\nSNGD\nSPRING\n0 2000 4000 6000 8000 10000\nIteration1011\n109\n107\n105\n103\n101\n101Solution error ||*||\n(J)=102, (H)=102\nSGD\nSNGD\nSPRING\nFigure 1: SGD, SNGD, and SPRING for two randomly generated difficult instances of Problem 1.\nAll methods have their hyperparameters tuned independently for each problem.\nCONCLUSIONS\nIn this work we have provided the first fast convergence rates for SNGD, leveraging ideas from\nrandomized linear algebra to explain its effectiveness for both linear least-squares and the more\ngeneral linear least-quadratics. We have also provided the first convergence analysis for SPRING,\nproving that it can accelerate SNGD for linear least-squares and explaining why its benefits are\namplified for small batch sizes. We emphasize that viewing these algorithms as randomized block\nKaczmarz methods has enabled us to obtain convergence guarantees for arbitrary batch sizes and\nhas also explained for the first time why it is beneficial to estimate both the stochastic gradient and\nthe stochastic preconditioner with a single batch of samples.\nLooking forward, the analysis of SPRING for linear least-quadratics represents a clear and important\ndirection for further research. It should also be very fruitful to study the convergence of other\nrecently proposed algorithms for training NNWs, such as those of Li et al. (2025); Armegioiu et al.\n(2025); Jiang et al. (2025), in the setting of LLQ. Going beyond existing algorithms, we expect\nthat the study of optimal row-access methods for LLQ can lead to the development of entirely\nnew algorithms which could ultimately inspire new state-of-the-art optimizers for neural network\nwavefunctions and other parametric optimization problems. Other promising directions include (i)\ngeneralizing our results to inconsistent or non-quadratic parametric optimization problems, and (ii)\nextending our analytical framework to study the effects of subsampling on optimizers other than\nnatural gradient descent.\n10\n\n--- Page 11 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nACKNOWLEDGEMENTS\nG.G. acknowledges support from the U.S. Department of Energy, Office of Science, Office of Ad-\nvanced Scientific Computing Research, Department of Energy Computational Science Graduate\nFellowship under Award Number DE-SC0023112. This work was supported in part by the U.S.\nDepartment of Energy, Office of Science, Office of Advanced Scientific Computing Research\u2019s Ap-\nplied Mathematics Competitive Portfolios program under Contract No. AC02-05CH11231 (L.L.),\nand by the Simons Investigator in Mathematics award through Grant No. 825053 (J.H., L.L.). We\nthank Ethan Epperly and Marius Zeinhofer for helpful discussions.\nREFERENCES\nNilin Abrahamsen, Zhiyan Ding, Gil Goldshlager, and Lin Lin. Convergence of variational Monte\nCarlo simulation and scale-invariant pre-training. Journal of Computational Physics , 513:113140,\n2024.\nShun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation , 10(2):251\u2013\n276, 1998.\nVictor Armegioiu, Juan Carrasquilla, Siddhartha Mishra, Johannes M \u00a8uller, Jannes Nys, Marius\nZeinhofer, and Hang Zhang. Functional neural wavefunction optimization. arXiv preprint\narXiv:2507.10835 , 2025.\nHilal Asi and John C Duchi. Stochastic (approximate) proximal point methods: Convergence, opti-\nmality, and adaptivity. SIAM Journal on Optimization , 29(3):2257\u20132290, 2019.\nDimitri P Bertsekas. Incremental proximal methods for large scale convex optimization. Mathemat-\nical programming , 129(2):163\u2013195, 2011.\nRaghu Bollapragada, Richard H Byrd, and Jorge Nocedal. Exact and inexact subsampled Newton\nmethods for optimization. IMA Journal of Numerical Analysis , 39(2):545\u2013578, 2019.\nL\u00b4eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine\nlearning. SIAM review , 60(2):223\u2013311, 2018.\nMarin Bukov, Markus Schmitt, and Maxime Dupont. Learning the ground state of a non-stoquastic\nquantum Hamiltonian in a rugged neural network landscape. SciPost Physics , 10(6):147, 2021.\nTianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua Zhang, and Liwei Wang.\nGram-Gauss-Newton method: Learning overparameterized neural networks for regression prob-\nlems. arXiv preprint arXiv:1905.11675 , 2019.\nGiuseppe Carleo and Matthias Troyer. Solving the quantum many-body problem with artificial\nneural networks. Science , 355(6325):602\u2013606, 2017.\nAo Chen and Markus Heyl. Empowering deep neural quantum states through efficient optimization.\nNature Physics , 20(9):1476\u20131481, 2024.\nFelix Dangel, Johannes M \u00a8uller, and Marius Zeinhofer. Kronecker-factored approximate curvature\nfor physics-informed neural networks. Advances in Neural Information Processing Systems , 37:\n34582\u201334636, 2024.\nDamek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex\nfunctions. SIAM Journal on Optimization , 29(1):207\u2013239, 2019.\nMicha\u0142 Derezi \u00b4nski and Elizaveta Rebrova. Sharp analysis of sketch-and-project methods via a con-\nnection to randomized singular value decomposition. SIAM Journal on Mathematics of Data\nScience , 6(1):127\u2013153, 2024.\nMicha\u0142 Derezi \u00b4nski, Daniel LeJeune, Deanna Needell, and Elizaveta Rebrova. Fine-grained analysis\nand faster algorithms for iteratively solving linear systems. arXiv preprint arXiv:2405.05818 ,\n2024.\n11\n\n--- Page 12 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nMicha\u0142 Derezi \u00b4nski, Deanna Needell, Elizaveta Rebrova, and Jiaming Yang. Randomized Kaczmarz\nmethods with beyond-Krylov convergence. arXiv preprint arXiv:2501.11673 , 2025.\nEthan N Epperly, Gil Goldshlager, and Robert J Webber. Randomized Kaczmarz with tail averaging.\narXiv preprint arXiv:2411.19877 , 2024.\nGuillaume Garrigos and Robert M Gower. Handbook of convergence theorems for (stochastic)\ngradient methods. arXiv preprint arXiv:2301.11235 , 2023.\nGil Goldshlager, Nilin Abrahamsen, and Lin Lin. A Kaczmarz-inspired approach to accelerate the\noptimization of neural network wavefunctions. Journal of Computational Physics , 516:113351,\n2024.\nGil Goldshlager, Jiang Hu, and Lin Lin. Worth their weight: Randomized and regularized block\nKaczmarz algorithms without preprocessing. arXiv preprint arXiv:2502.00882 , 2025.\nRobert Gower, Filip Hanzely, Peter Richt \u00b4arik, and Sebastian U Stich. Accelerated stochastic ma-\ntrix inversion: general theory and speeding up BFGS rules for faster second-order optimization.\nAdvances in Neural Information Processing Systems , 31, 2018.\nRobert M Gower and Peter Richt \u00b4arik. Randomized iterative methods for linear systems. SIAM\nJournal on Matrix Analysis and Applications , 36(4):1660\u20131690, 2015.\nYuntian Gu, Wenrui Li, Heng Lin, Bo Zhan, Ruichen Li, Yifei Huang, Di He, Yantao Wu, Tao\nXiang, Mingpu Qin, et al. Solving the Hubbard model with neural quantum states. arXiv preprint\narXiv:2507.02644 , 2025.\nAndr \u00b4es Guzm \u00b4an-Cordero, Felix Dangel, Gil Goldshlager, and Marius Zeinhofer. Improving en-\nergy natural gradient descent through woodbury, momentum, and randomization. arXiv preprint\narXiv:2505.12149 , 2025.\nJan Hermann, Zeno Sch \u00a8atzle, and Frank No \u00b4e. Deep-neural-network solution of the electronic\nSchr \u00a8odinger equation. Nature Chemistry , 12(10):891\u2013897, 2020.\nPrateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Paralleliz-\ning stochastic gradient descent for least squares regression: mini-batching, averaging, and model\nmisspecification. Journal of machine learning research , 18(223):1\u201342, 2018.\nDu Jiang, Xuelan Wen, Yixiao Chen, Ruichen Li, Weizhong Fu, Hung Q Pham, Ji Chen, Di He,\nWilliam A Goddard III, Liwei Wang, et al. Neural scaling laws surpass chemical accuracy for the\nmany-electron Schr \u00a8odinger equation. arXiv preprint arXiv:2508.02570 , 2025.\nAditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Char-\nacterizing possible failure modes in physics-informed neural networks. Advances in neural infor-\nmation processing systems , 34:26548\u201326560, 2021.\nChenyi Li, Shuchen Zhu, Zhonglin Xie, and Zaiwen Wen. Accelerated natural gradient method for\nparametric manifold optimization. arXiv preprint arXiv:2504.05753 , 2025.\nTianyou Li, Fan Chen, Huajie Chen, and Zaiwen Wen. Convergence analysis of stochastic gradient\ndescent with MCMC estimators. arXiv preprint arXiv:2303.10599 , 2023.\nXiang Li, Zhe Li, and Ji Chen. Ab initio calculation of real solids via neural network ansatz. Nature\nCommunications , 13(1):7895, 2022.\nSiyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effec-\ntiveness of sgd in modern over-parametrized learning. In International Conference on Machine\nLearning , pp. 3325\u20133334. PMLR, 2018.\nJames Martens. New insights and perspectives on the natural gradient method. Journal of Machine\nLearning Research , 21(146):1\u201376, 2020.\nSi Yi Meng, Sharan Vaswani, Issam Hadj Laradji, Mark Schmidt, and Simon Lacoste-Julien. Fast\nand furious convergence: Stochastic second order methods under interpolation. In International\nConference on Artificial Intelligence and Statistics , pp. 1375\u20131386. PMLR, 2020.\n12\n\n--- Page 13 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nJohannes M \u00a8uller and Marius Zeinhofer. Achieving high accuracy with PINNs via energy natural\ngradient descent. In International Conference on Machine Learning , pp. 25471\u201325485. PMLR,\n2023.\nDeanna Needell and Joel A Tropp. Paved with good intentions: analysis of a randomized block\nKaczmarz method. Linear Algebra and its Applications , 441:199\u2013221, 2014.\nDeanna Needell, Nathan Srebro, and Rachel Ward. Stochastic gradient descent, weighted sampling,\nand the randomized Kaczmarz algorithm. Advances in neural information processing systems ,\n27, 2014.\nChae-Yeun Park and Michael J Kastoryano. Geometry of learning neural quantum states. Physical\nReview Research , 2(2):023232, 2020.\nDavid Pfau, James S Spencer, Alexander GDG Matthews, and W Matthew C Foulkes. Ab initio\nsolution of the many-electron Schr \u00a8odinger equation with deep neural networks. Physical review\nresearch , 2(3):033429, 2020.\nMaziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A\ndeep learning framework for solving forward and inverse problems involving nonlinear partial\ndifferential equations. Journal of Computational physics , 378:686\u2013707, 2019.\nPratik Rathore, Weimu Lei, Zachary Frangella, Lu Lu, and Madeleine Udell. Challenges in training\nPINNs: A loss landscape perspective. arXiv preprint arXiv:2402.01868 , 2024.\nYi Ren and Donald Goldfarb. Efficient subsampled Gauss-Newton and natural gradient methods for\ntraining neural networks. arXiv preprint arXiv:1906.02353 , 2019.\nRiccardo Rende, Luciano Loris Viteritti, Lorenzo Bardone, Federico Becca, and Sebastian Goldt. A\nsimple linear algebra identity to optimize large-scale neural network quantum states. Communi-\ncations Physics , 7(1):260, 2024.\nPeter Richt \u00b4arik and Martin Tak \u00b4ac. Stochastic reformulations of linear systems: algorithms and\nconvergence theory. SIAM Journal on Matrix Analysis and Applications , 41(2):487\u2013524, 2020.\nFarbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled newton methods i: globally con-\nvergent algorithms. arXiv preprint arXiv:1601.04737 , 2016a.\nFarbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled newton methods ii: Local con-\nvergence rates. arXiv preprint arXiv:1601.04738 , 2016b.\nMichael Scherbela, Nicholas Gao, Philipp Grohs, and Stephan G \u00a8unnemann. Accurate ab-\ninitio neural-network solutions to large-scale electronic structure problems. arXiv preprint\narXiv:2504.06087 , 2025.\nConor Smith, Yixiao Chen, Ryan Levy, Yubo Yang, Miguel A Morales, and Shiwei Zhang. Uni-\nfied variational approach description of ground-state phases of the two-dimensional electron gas.\nPhysical Review Letters , 133(26):266504, 2024.\nSandro Sorella. Generalized Lanczos algorithm for variational quantum Monte Carlo. Physical\nReview B , 64(2):024512, 2001.\nSifan Wang, Xinling Yu, and Paris Perdikaris. When and why PINNs fail to train: A neural tangent\nkernel perspective. Journal of Computational Physics , 449:110768, 2022.\nRobert J Webber and Michael Lindsey. Rayleigh-Gauss-Newton optimization with enhanced sam-\npling for variational Monte Carlo. Physical Review Research , 4(3):033099, 2022.\nJiayuan Wu, Jiang Hu, Hongchao Zhang, and Zaiwen Wen. Convergence analysis of an adaptively\nregularized natural gradient method. IEEE Transactions on Signal Processing , 72:2527\u20132542,\n2024.\nXianliang Xu, Ting Du, Wang Kong, Bin Shan, Ye Li, and Zhongyi Huang. Convergence analysis of\nnatural gradient descent for over-parameterized physics-informed neural networks. arXiv preprint\narXiv:2408.00573 , 2024.\n13\n\n--- Page 14 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nMinghan Yang, Dong Xu, Zaiwen Wen, Mengyun Chen, and Pengxiang Xu. Sketchy empirical\nnatural gradient methods for deep learning. arXiv preprint arXiv:2006.05924 , 2020.\nGuodong Zhang, James Martens, and Roger B Grosse. Fast convergence of natural gradient descent\nfor over-parameterized neural networks. Advances in Neural Information Processing Systems , 32,\n2019.\nA P ROOFS FOR LINEAR LEAST -SQUARES\nProof of Theorem 1. The equivalence between SNGD and regularized Kaczmarz is direct; compare\neq. (11) with for example eq. (1.7) of Goldshlager et al. (2025).\nFor the case of SPRING and ARK, we first reproduce the ARK method as defined by Derezi \u00b4nski\net al. (2025):\nwt=J+(\u03bb)\nS(JS\u02dc\u03b8t\u2212bS),\n\u02dc\u03d5t+1=\u00b5(\u02dc\u03d5t\u2212wt),\n\u02dc\u03b8t+1=\u02dc\u03b8t\u2212wt+ \u02dc\u03b7\u02dc\u03d5t+1(25)\nWe use the symbols \u02dc\u03b8,\u02dc\u03d5,\u02dc\u03b7since these variables do not correspond directly to the variables \u03b8, \u03d5, \u03b7 of\nSPRING. Indeed, we now show that the SPRING algorithm with initial guess \u03b80, initial momentum\n\u03d50= 0, momentum coefficient \u00b5, and step size \u03b7is equivalent to the ARK algorithm eq. (25) with\ninitial guess \u02dc\u03b80=\u03b80, initial momentum \u02dc\u03d50= 0, momentum coefficient \u00b5, and step size \u02dc\u03b7= 1\u22121\u2212\u03b7\n\u00b5\nin that the resulting iterates satisfy \u02dc\u03d5t=\u2212\u00b5\u03d5t,\u02dc\u03b8t=\u03b8t\u2212\u00b5\u03d5t.\nProceeding by induction, as a base case we have \u02dc\u03d50=\u2212\u00b5\u03d50= 0and\u02dc\u03b80=\u03b80\u2212\u00b5\u03d50=\u03b80. Now,\nsuppose these identities hold for some fixed t\u22650. Then calculate using eq. (25) and eq. (10)\n\u02dc\u03d5t+1=\u00b5(\u02dc\u03d5t\u2212J+(\u03bb)\nS(JS\u02dc\u03b8t\u2212bs))\n=\u2212\u00b5(\u00b5\u03d5t+J+(\u03bb)\nS(JS(\u03b8t\u2212\u00b5\u03d5t)\u2212bS))\n=\u2212\u00b5(\u00b5\u03d5t+J+(\u03bb)\nS(JS\u03b8t\u2212bS\u2212\u00b5JS\u03d5t))\n=\u2212\u00b5\u03d5t+1.\nUsing the above, proceed to verify\n\u02dc\u03b8t+1=\u02dc\u03b8t\u2212J+(\u03bb)\nS(JS\u02dc\u03b8t\u2212bs) + \u02dc\u03b7\u02dc\u03d5t+1\n=\u03b8t\u2212\u00b5\u03d5t\u2212J+(\u03bb)\nS(JS\u03b8t\u2212bS\u2212\u00b5JS\u03d5t)\u2212\u0012\n1\u22121\u2212\u03b7\n\u00b5\u0013\n\u00b5\u03d5t+1\n=\u03b8t\u2212\u03d5t+1\u2212\u00b5\u03d5t+1+ (1\u2212\u03b7)\u03d5t+1\n= (\u03b8t\u2212\u03b7\u03d5t+1)\u2212\u00b5\u03d5t+1\n=\u03b8t+1\u2212\u00b5\u03d5t+1.\nThis completes the inductive step.\nThe convergence rate for SNGD is similar to previous analyses of regularized Kaczmarz methods\nGoldshlager et al. (2025); Derezi \u00b4nski et al. (2025), but for the convenience of the reader we provide\nan outline of the proof. Using consistency J\u03b8\u2217=bthe SNGD iteration for LLS is rewritten as\n\u03b8t+1\u2212\u03b8\u2217= (I\u2212P(S))(\u03b8t\u2212\u03b8\u2217). (26)\nFrom here the expected squared error can evaluated:\nE\u2225\u03b8t+1\u2212\u03b8\u2217\u22252= (\u03b8t\u2212\u03b8\u2217)\u22a4E\u0002\n(I\u2212P(S))2\u0003\n(\u03b8t\u2212\u03b8\u2217) (27)\nThe expectation can then be bounded using P(S)2\u2aafP(S), yielding\nE\u0002\n(I\u2212P(S))2\u0003\n\u2aafI\u22122P+P=I\u2212P. (28)\nThe convergence rate of (1\u2212\u03b1)tfollows.\nFor SPRING, we invoke previous analyses of ARK to obtain the convergence bound. See for exam-\nple equation (1) of (Derezi \u00b4nski et al., 2024), which presents the result in a way that is most similar\nto our own, and (Gower et al., 2018; Richt \u00b4arik & Tak \u00b4ac, 2020) for previous discussions.\n14\n\n--- Page 15 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nB P ROOFS FOR LLQ\nProof of Proposition 3. Recall that under Assumption 2 the SNGD iteration can be written as\n\u03b8t+1=\u03b8t\u2212\u03b7P(S)J+(HJ\u03b8 t+q). (29)\nSubtracting \u03b8\u2217from both sides and using HJ\u03b8 +q=HJ(\u03b8\u2212\u03b8\u2217)it follows\n(\u03b8t+1\u2212\u03b8\u2217) = (I\u2212\u03b7P(S)J+HJ)(\u03b8t\u2212\u03b8\u2217). (30)\nThe expected error thus evolves as\nE[\u03b8t+1\u2212\u03b8\u2217] = (I\u2212\u03b7PJ+HJ)(\u03b8t\u2212\u03b8\u2217)\n= (I\u2212\u03b7M)(\u03b8t\u2212\u03b8\u2217),\nand iterating this calculation implies\nE[\u03b8t\u2212\u03b8\u2217] = (I\u2212\u03b7M)t(\u03b80\u2212\u03b8\u2217). (31)\nIf\u03be(\u03bb)>0andMthus has an eigenvalue with negative real part, it follows that there exists an\neigenvalue of I\u2212\u03b7Mthat lies outside the unit circle in the complex plane for any choice of \u03b7 >0,\nleading the expectation of the SNGD iterate to diverge when the initial guess contains a component\nof the corresponding eigenvector. This proves the first statement of the proposition.\nRegarding the second statement, note first that\nlim\n\u03bb\u2192\u221e\u03bbP= lim\n\u03bb\u2192\u221e\u03bbES\u0002\nJ\u22a4\nS(JSJ\u22a4\nS+\u03bbI)\u22121JS\u0003\n= lim\n\u03bb\u2192\u221eES\u0002\nJ\u22a4\nS(I+JSJ\u22a4\nS/\u03bb)\u22121JS\u0003\n=k\nmJ\u22a4J.\n(32)\nIt follows that\nlim\n\u03bb\u2192\u221e\u03bbM= lim\n\u03bb\u2192\u221e\u03bbPJ+HJ=k\nmJ\u22a4HJ\u2ab00. (33)\nSimilarly\nlim\n\u03bb\u2192\u221e\u03bb(M\u22a4+M) =2k\nmJ\u22a4HJ\u2ab00, (34)\nand so there exists \u03bb0such that for \u03bb > \u03bb 0it holds M\u22a4+M\u2ab00.\nNext, write the expected squared error of SNGD in terms of M:\nE\u2225\u03b8t+1\u2212\u03b8\u2217\u22252= (\u03b8t\u2212\u03b8\u2217)\u22a4(I\u2212P(S)J+HJ)\u22a4(I\u2212P(S)J+HJ)(\u03b8t\u2212\u03b8\u2217)\n= (\u03b8t\u2212\u03b8\u2217)\u22a4\u0000\nI\u2212\u03b7(M\u22a4+M) +\u03b72B\u0001\n(\u03b8t\u2212\u03b8\u2217)\nwhere B=ES\u0002\n(J+HJ)\u22a4P(S)2J+HJ\u0003\n. For\u03bb > \u03bb 0we already have M\u22a4+M\u2ab00and thus for\nsmall enough \u03b7it holds\n\nI\u2212\u03b7(M\u22a4+M) +\u03b72B\n\n<1, making SNGD convergent.\nProposition 8 (Convergence of NGD) .Consider a consistent LLQ problem J\u03b8\u2217= arg minvL(v)\nand let \u02dcH= (J\u22a4J)\u22121/2J\u22a4HJ(J\u22a4J)\u22121/2be the projection of Honto the column space of J.\nThen the NGD algorithm with step size \u03b7= 1/\u03bbmax(\u02dcH)and regularization \u03bb= 0satisfies the error\nbound\n\u2225\u03b8t\u2212\u03b8\u2217\u2225J\u22a4J\u2264(1\u2212\u03ba\u22121(\u02dcH))t\u2225\u03b80\u2212\u03b8\u2217\u2225J\u22a4J. (35)\nCompared with the gradient descent, the rate is improved from 1\u2212\u03ba\u22121(JTHJ)to1\u2212\u03ba\u22121(\u02dcH),\neffectively removing the conditioning of Jfrom the problem and confirming the standard intuition\nbehind NGD. The proof of Proposition 8 can be found in Appendix B.\nProof of Proposition 8. Since J\u03b8\u2217= arg minvL(v)it holds HJ\u03b8 +q=HJ(\u03b8\u2212\u03b8\u2217).Consider the\nevolution of the error in \u03b8:\n\u03b8t+1\u2212\u03b8\u2217=\u03b8t\u2212\u03b7J+(HJ\u03b8 t+q)\u2212\u03b8\u2217\n= (\u03b8t\u2212\u03b8\u2217)\u2212\u03b7J+HJ(\u03b8t\u2212\u03b8\u2217)\n= (I\u2212\u03b7J+HJ)(\u03b8t\u2212\u03b8\u2217).\n15\n\n--- Page 16 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nMultiplying on the left by (J\u22a4J)1/2and inserting (J\u22a4J)\u22121/2(J\u22a4J)1/2between the two factors on\nthe right hand side, we obtain\n(J\u22a4J)1/2(\u03b8t+1\u2212\u03b8\u2217) =\u0010\nI\u2212\u03b7(J\u22a4J)1/2J+HJ(J\u22a4J)\u22121/2\u0011\n(J\u22a4J)1/2(\u03b8t\u2212\u03b8\u2217)\n= (I\u2212\u03b7\u02dcH)(JTJ)1/2(\u03b8t\u2212\u03b8\u2217).\nBy setting \u03b7= 1/\u03bbmax(\u02dcH)and taking squared norms of both sides it follows that\n\u2225\u03b8t+1\u2212\u03b8\u2217\u22252\nJ\u22a4J\u2264(1\u2212\u03ba\u22121(\u02dcH))2\u2225\u03b8t\u2212\u03b8\u2217\u22252\nJ\u22a4J. (36)\nTaking the square root and iterating this result yields the proposition.\nProposition 9 (Sufficient conditions for Theorem 4) .The condition that Pshares an eigenbasis\nwithJ\u22a4Jholds in two special cases:\n1. For arbitrary J, the condition holds when the block Sis sampled from the determinantal\npoint process k-DPP( JJ\u22a4+\u03bbI), namely\nPr[S] =det(JSJ\u22a4\nS+\u03bbI)P\n|S\u2032|=kdet(JS\u2032J\u22a4\nS\u2032+\u03bbI). (37)\n2. When the rows of Jarise from a Gaussian distribution, the condition holds asymptotically\nasm\u2192 \u221e .\nProof. For the case of DPP sampling, the expectation matrix Pcan be calculated explicitly and is\nknown to share an eigenbasis with J\u22a4J; see for example eq. (E.7) of Goldshlager et al. (2024).\nFor the case of Gaussian data, it is known that in the limit m\u2192 \u221e , a uniform sample from a\nGaussian data matrix Jbecomes equivalent to a Gaussian sketch of the Cholesky factor Lof the\ncovariance matrix of the Gaussian distribution; see the proof of Theorem 3.2 in Goldshlager et al.\n(2025). It is also known that the expected projection matrix for a Gaussian sketch of Lshares an\neigenbasis with LTL; this is for example a consequence of Lemma 4.1 of Derezi \u00b4nski et al. (2024).\nFinally, in the limit m\u2192 \u221e the covariance estimator1\nmJ\u22a4Jconverges to the covariance matrix\nL\u22a4L. Thus P,L\u22a4L, and J\u22a4Jall share an eigenbasis in the limit.\nProof of Theorem 4. First, the bound \u03b3\u2265\u03ba\u22121(Q)holds since\nEh\n(J\u22a4J)\u22121/2P(S)Q\u22121P(S)(J\u22a4J)\u22121/2i\n\u2aaf1\n\u03bbmin(Q)Eh\n(J\u22a4J)\u22121/2P(S)2(J\u22a4J)\u22121/2i\n\u2aaf1\n\u03bbmin(Q)Eh\n(J\u22a4J)\u22121/2P(J\u22a4J)\u22121/2i\n=1\n\u03bbmin(Q)Q\n\u2aaf\u03ba(Q)I.\nFor the SNGD convergence bound, recall from the proof of Proposition 3 that the SNGD update can\nbe written as\n\u03b8t+1\u2212\u03b8\u2217= (I\u2212\u03b7P(S)J+HJ)(\u03b8t\u2212\u03b8\u2217). (38)\nDefine Q=P(S)(J\u22a4J)\u22121so that this can be condensed to\n\u03b8t+1\u2212\u03b8\u2217= (I\u2212\u03b7QJ\u22a4HJ)(\u03b8t\u2212\u03b8\u2217). (39)\nNote that E[Q] :=Q=P(J\u22a4J)\u22121= (J\u22a4J)\u22121/2P(J\u22a4J)\u22121/2\u2ab00. We exploit this property by\ntaking the expectation of the squared Q\u22121norm of both sides, yielding\nE\u2225\u03b8t+1\u2212\u03b8\u2217\u22252\nQ\u22121= (\u03b8t\u2212\u03b8\u2217)\u22a4(Q\u22121\u22122\u03b7J\u22a4HJ+\u03b72J\u22a4HJEh\nQ\u22a4Q\u22121Qi\nJ\u22a4HJ)(\u03b8t\u2212\u03b8\u2217).\n(40)\n16\n\n--- Page 17 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nThe choice of the Q\u22121norm has ensured that every term in this expression is positive definite, which\nis the key to making the analysis go through. For the quadratic term note\nEh\nQ\u22a4Q\u22121Qi\n= (J\u22a4J)\u22121/2Eh\n(J\u22a4J)\u22121/2P(S)Q\u22121P(S)(J\u22a4J)\u22121/2i\n(J\u22a4J)\u22121/2\n\u2aaf(J\u22a4J)\u22121/\u03b3.\nThus when \u03b7=\u03b3/\u03bbmax(\u02dcH)it holds\n\u03b72J\u22a4HJEh\nQ\u22a4Q\u22121Qi\nJ\u22a4HJ\u2aaf\u03b72J\u22a4HJ(J\u22a4J)\u22121J\u22a4HJ/\u03b3\n\u2aaf\u03b72J\u22a4H2J/\u03b3\n\u2aaf\u03b7J\u22a4HJ.\nReturning to the expected squared error and substituting this result yields\nE\u2225\u03b8t+1\u2212\u03b8\u2217\u22252\nQ\u22121\u2264(\u03b8t\u2212\u03b8\u2217)\u22a4(Q\u22121\u2212\u03b7J\u22a4HJ)(\u03b8t\u2212\u03b8\u2217)\n\u2264(\u03b8t\u2212\u03b8\u2217)\u22a4(Q\u22121\u2212\u03ba\u22121(\u02dcH)\u00b7(J\u22a4J)\u00b7\u03b3)(\u03b8t\u2212\u03b8\u2217)\n\u2264(\u03b8t\u2212\u03b8\u2217)\u22a4(Q\u22121\u2212\u03ba\u22121(\u02dcH)\u00b7PQ\u22121\u00b7\u03b3)(\u03b8t\u2212\u03b8\u2217)\n= (1\u2212\u03ba\u22121(\u02dcH)\u00b7\u03b1\u00b7\u03b3)\u2225\u03b8t\u2212\u03b8\u2217\u2225Q\u22121.\nThe main result follows by iterating this bound.\nTo prove Corollary 5, we need the following result regarding the ordering of the diagonal entries of\nthe expected projector Pof a diagonal matrix:\nLemma 10. LetD= diag( d1, . . . , d n)withd1\u2265. . .\u2265dn. Let P=E[(ZD)+(ZD)]where Z\nis ak\u00d7nrandom Gaussian sketch matrix. Then Pis diagonal by Proposition 9. Furthermore, its\ndiagonal entries p1, . . . , p nsatisfy p1\u2265. . .\u2265pn.\nProof of Lemma 10. In general, the diagonal entry pitakes the form\npi=e\u22a4\niE\u0002\nDZ\u22a4(ZD2Z\u22a4)+(ZD)\u0003\nei=d2\ni\u00b7E\u0002\nz\u22a4\ni(ZD2Z\u22a4)+zi\u0003\n(41)\nwhere ziis column iofZ. Now fix some i < j and consider \u02dcD=D\u2212eie\u22a4\ni(di\u2212dj), so that the\ndiagonal entries of \u02dcDare the same as Dexcept that its ith entry is equal to djinstead of di. Similarly\nlet\u02dcP=Eh\n(Z\u02dcD)+(Z\u02dcD)i\n. Note that D\u2ab0\u02dcDand symmetry it holds that \u02dcpi= \u02dcpj. Furthermore, for\nany\u2113\u0338=iit holds\n\u02dcp\u2113=\u02dcd2\ni\u00b7Eh\nz\u22a4\ni(Z\u02dcD2Z\u22a4)+zii\n\u2265d2\ni\u00b7E\u0002\nz\u22a4\ni(ZD2Z\u22a4)+zi\u0003\n=p\u2113 (42)\nusing \u02dcdi=diandD\u2ab0\u02dcD. Additionally, it is easy to verify that Tr(D) = Tr( \u02dcD) =kwhich implies\n\u02dcpi\u2264pito keep the trace constant. Putting these results together we have\npj\u2264\u02dcpj= \u02dcpi\u2264pi, (43)\nand since this holds for arbitrary i < j it follows p1\u2265. . .\u2265pn.\nWe can now prove the corollary:\nProof of Corollary 5. We first prove the convergence rate for SNGD. It has been shown in (Gold-\nshlager et al., 2025, Corollary 3.3) that under the conditions of Corollary 5, \u03b1= \u2126( k\u03b2\u03ba\u22121\ndem(J)).\nTo bound \u03b3, we first recall \u03b3\u2265\u03ba\u22121(Q)from Theorem 4 and thus turn our attention to bound-\ning\u03ba(Q). Under our assumption of Gaussian data, it is known that a uniform row sample from J\nbecomes equivalent to a random Gaussian sketch of Lwhere \u03a3 = L\u22a4Lis the Cholesky decom-\nposition of the covariance matrix (Goldshlager et al., 2025, proof of Theorem 3.2). Additionally,\nLemma 4.1 of Derezi \u00b4nski & Rebrova (2024) shows replacing Lwith the diagonal matrix of its sin-\ngular values does not change the spectrum of P. Using the commutation result of Proposition 9 and\n17\n\n--- Page 18 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nQ=P(J\u22a4J)\u22121=1\nmP(L\u22a4L)\u22121it follows that replacing Lwith the diagonal matrix of its singular\nvalues does not change the spectrum of Qeither. As a result, it suffices to prove a bound on \u03ba(Q)in\nthe simplified setting of Gaussian sketches of a diagonal matrix D.\nAssume without loss of generality that D= diag( d1, . . . , d n)where d1\u2265. . .\u2265dn. Furthermore,\nletP= diag( p1, . . . , p n)and recall by Lemma 10 that p1\u2265. . . p n. It then holds that \u03ba(Q) =\n(pi/d2\ni)/(pj/d2\nj)for some i, j. Then if i\u2264jwe can bound \u03ba(Q)\u2264pi/pj\u2264\u03ba(P), whereas if\ni\u2265jwe can bound \u03ba(Q)\u2264d2\nj/d2\ni\u2264\u03ba2(D). Translating back to the original problem, it follows\nthat\u03ba(Q)\u2264max( \u03ba(P), \u03ba2(J)). In the Gaussian case with fast spectral decay, \u03ba2(J)is on the same\norder as \u03ba2\ndem(J)and\u03ba(P)\u22641/\u03b1\u226a\u03ba2\ndem(J), so it follows \u03ba\u22121(Q) = \u2126( \u03ba\u22122(J)). Combining\nthis with the earlier bound on \u03b1we have \u03b1\u00b7\u03b3= \u2126( k\u03b2\u03ba\u22122\ndem(J)\u00b7\u03ba\u22122(J)), and the rate of SNGD\nfollows.\nWe next analyze the convergence rate of mini-batch SGD. First consider the standard SGD update\nwith batch size k= 1, given by\n\u03b8t+1=\u03b8t\u2212\u03b7\nwiJ\u22a4\ni(HiJ\u03b8t+qi), (44)\nwhere Ji\u2208R1\u00d7nandHi\u2208Rdenote the i-th row of the matrices JandH, respectively. The scalar\nwi>0is a non-negative weight, and \u03b7 >0is the step size.\nFollowing Needell et al. (2014), we set wiproportional to the local Lipschitz constants, specifically\nwi=Li\n\u00afL, L i:=\u2225J\u22a4\niHiJ\u22252,\u00afL:=1\nmmX\ni=1Li.\nWith this choice, we have\nE(w)[\u2225\u03b8t+1\u2212\u03b8\u2217\u22252] =E(w)h\n\u2225\u03b8t\u2212\u03b7\nwiJ\u22a4\ni(HiJ\u03b8t+qi)\u2212\u03b8\u2217\u22252i\n=\u2225\u03b8t\u2212\u03b8\u2217\u22252\u22122E(w)h\n\u03b8t\u2212\u03b8\u2217,\u03b7\nwiJ\u22a4\ni(HiJ\u03b8t+qi)\u000bi\n+E(w)h\n\u03b72\nw2\ni\u2225J\u22a4\ni(HiJ\u03b8t+qi)\u22252i\n.\nAt this stage, the classical analysis Needell et al. (2014); Garrigos & Gower (2023) relies on convex-\nity, but here J\u22a4\niHiJis asymmetric and does not correspond to the gradient of any convex component\nfunction. Using instead the estimator\u2019s unbiasedness together with the local Lipschitz constant Li\n(which defines wi), we obtain\nE(w)[\u2225\u03b8t+1\u2212\u03b8\u2217\u22252]\u2264 \u2225\u03b8t\u2212\u03b8\u2217\u22252\u22122\u03b7\nm\n\u03b8t\u2212\u03b8\u2217, J\u22a4(HJ\u03b8 t+q)\u000b\n+\n\n\nE(w)h\n2\u03b72\nw2\ni(J\u22a4\niHiJ)\u22a4(J\u22a4\niHiJ)i\n\n\n2\u2225\u03b8t\u2212\u03b8\u2217\u22252\n+E(w)h\n2\u03b72\nw2\ni\u2225J\u22a4\ni(HiJ\u03b8\u2217+qi)\u22252i\n\u2264\u0010\n1\u22122\u03b7\u03bbmin(J\u22a4HJ)\nm\u22122\u03b72\u02c6L2\u0011\n\u2225\u03b8t\u2212\u03b8\u2217\u22252,\nwhere \u02c6L2:=\n\n\nE(w)h\n1\nw2\ni(J\u22a4\niHiJ)\u22a4(J\u22a4\niHiJ)i\n\n\n2=\n\n\nEh\n1\nwi(J\u22a4\niHiJ)\u22a4(J\u22a4\niHiJ)i\n\n\n2and we assume\nHJ\u03b8\u2217+q= 0. The optimal rate is then achieved at step size \u03b7=\u00b5\n2m\u02c6L2, and the SGD iteration in\neq. (44) satisfies\nE\u2225\u03b8t+1\u2212\u03b8\u2217\u22252\u2264(1\u2212\u03b1SGD)\u2225\u03b80\u2212\u03b8\u2217\u22252where \u03b1SGD=\u03bb2\nmin(J\u22a4HJ)\n2m2\u02c6L2. (45)\n18\n\n--- Page 19 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nTo obtain an upper bound for \u03b1SGD we must lower bound \u02c6L2. To this end define \u02dcC=\n1\n2max i\u2225(\u02dcHJ)i\u2225\n\u2225\u02dcH\u22252\u2225Ji\u2225. Then,\n\u02c6L2=\n\n\nEh\n1\nwiJ\u22a4H\u22a4\niJiJ\u22a4\niHiJi\n\n\n2=\u00afL\n\n\n\nJ\u22a4E\u0014\nH\u22a4\niHi\u2225Ji\u22252\n2\n\u2225J\u22a4\ni(HJ)i\u22252\u0015\nJ\n\n\n\n2\n\u2265\u00afL\n\n\n\nJ\u22a4E\u0014\nH\u22a4\niHi\u2225Ji\u22252\n\u2225(\u02dcHJ)i\u22252\u0015\nJ\n\n\n\n2\u2265\u00afL\n2\u02dcC\u2225\u02dcH\u22252\u2225J\u22a4E[H\u22a4\niHi]J\u22252\n=\u00afL\n2m\u02dcC\u2225\u02dcH\u22252\u2225J\u22a4H2J\u22252\u2265\u00afL\n2m\u02dcC\u03ba(\u02dcH)\u2225J\u22a4HJ\u22252,\nwhere we use Assumption 2 to argue that \u02dcHJ=HJ in the first inequality and \u2225J\u22a4H2J\u22252\u2265\n\u03bbmin(\u02dcH)\u2225JTHJ\u22252in the last inequality. Substituting this bound into eq. (45), the convergence\nparameter is bounded by\n\u03b1SGD\u2264\u02dcC\u03ba(\u02dcH)\u00b7\u03bbmin(J\u22a4HJ)\nmL\u00b7\u03ba\u22121(J\u22a4HJ).\nWhen HiJJ\u22a4\ni\u22650, we have Li=HiJJ\u22a4\ni= tr( J\u22a4\niHiJ), which yields\n\u00afL=1\nmmX\ni=1tr(J\u22a4\niHiJ) =1\nmtr(J\u22a4HJ) =1\nm\u2225(J\u22a4HJ)1/2\u22252\nF.\nThus, \u00afL\u22651\nm\u2225(J\u22a4HJ)1/2\u22252\nFand for k= 1we obtain the final bound\n\u03b1SGD\u2264\u02dcC\u03ba(\u02dcH)\u00b7\u03ba\u22122\ndem\u0000\n(J\u22a4HJ)1/2\u0001\n\u00b7\u03ba\u22121(J\u22a4HJ). (46)\nLastly, for mini-batch SGD with batch size k >1it is shown in Jain et al. (2018) that the conver-\ngence rate can scale at most linearly with k, which implies that in general\n\u03b1SGD\u2264\u02dcC\u03ba(\u02dcH)\u00b7k\u03ba\u22122\ndem\u0000\n(J\u22a4HJ)1/2\u0001\n\u00b7\u03ba\u22121(J\u22a4HJ).\nProof of Proposition 6. First, when k=mand\u03bb= 0 SPRING becomes equivalent to NGD since\nin this case J+(\u03bb)\nSJS=J+J=Iand thus the momentum update takes the form\n\u03d5t+1=\u00b5\u03d5t+J+(HJ\u03b8 t+q\u2212\u00b5J\u03d5 t)\n=\u00b5(I\u2212J+J)\u03d5t+J+(HJ\u03b8 t+q)\n=J+(HJ\u03b8 t+q).\nIt follows that in this case SPRING converges as\n\u2225\u03b8t\u2212\u03b8\u2217\u2225J\u22a4J\u2264(1\u2212\u03ba\u22121(\u02dcH))t\u2225\u03b80\u2212\u03b8\u2217\u2225J\u22a4J. (47)\nSecond, when H=IProblem 1 reduces to Problem 2 and thus in this case SPRING satisfies the\nconvergence bound\nE\u2225\u03b8t\u2212\u03b8\u2217\u22252=O\u0010\n(1\u2212p\n\u03b1/\u03b2)t\u0011\n\u2225\u03b80\u2212\u03b8\u2217\u22252. (48)\nNoting that when k=mwe have \u03b1=\u03b2= 1and when H=Iwe have \u03ba\u22121(\u02dcH) = 1 we can unite\nthese two rates by putting them both in the form required by the proposition, namely\nE\u2225\u03b8t\u2212\u03b8\u2217\u22252=O\u0010\n(1\u2212\u03ba\u22121(\u02dcH)p\n\u03b1/\u03b2)t\u0011\n\u2225\u03b80\u2212\u03b8\u2217\u22252. (49)\n19\n\n--- Page 20 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nC E IGENVALUES OF THE MMATRIX\nIn this appendix we provide some numerical experiments regarding the eigenvalues of the Mmatrix\nfrom Proposition 3. First, we generate random Gaussian problems of two sizes, set \u03bb= 0, and plot\nthe corresponding eigenvalues of Min Figure 2. The results show that eigenvalues with negative\nreal parts appear commonly for the smallest problems ( m= 4,n= 2) but are less than one-in-a-\nthousand for even slightly larger problems ( m= 100 ,n= 10 ). In both cases we set k= 1to keep\nthe cost of calculating Mlinear in m.\nNext, in Figure 3 we demonstrate that increasing \u03bbshifts the eigenvalues of Mtowards having\npositive real parts, as expected from Proposition 3. The fact that we need to choose the very large\nregularization amount of \u03bb= 1 to observe this phenomenon is likely an artifact of the fact that we\nuse the tiny problem size of m= 4,n= 2for this experiment.\nFigure 2: Eigenvalues of Mfor randomly generated problems of two sizes. In both cases k= 1and\n\u03bb= 0. The imaginary axis is painted in blue in both pictures, eigenvalues with positive real parts\nare painted in green, and eigenvalues with negative real parts are painted in red. Eigenvalues are\ndisplayed for 1,000randomly generated problems in each panel.\nFigure 3: Eigenvalues of Mfor randomly generated problems with two different values of \u03bb. In\nboth cases (m, n, k ) = (4 ,2,1). The imaginary axis is painted in blue in both pictures, eigenvalues\nwith positive real parts are painted in green, and eigenvalues with negative real parts are painted in\nred. Eigenvalues are displayed for 1,000randomly generated problems in each panel.\n20\n\n--- Page 21 ---\nFast Convergence Rates for Subsampled Natural Gradient Algorithms\nD N UMERICAL EXPERIMENTS FOR LLQ\nFor Figure 1, we set \u03bb= 0 to keep things simple and focus on the effect of introducing the \u00b5\nparameter in SPRING. We set m= 10,000,n= 100 ,k= 10 and test the performance of SNGD\nand SPRING for random Gaussian problems with different condition numbers. We ensure that\nAssumption 2 hold by construction in all cases.\nAs an additional exploration, we also provide experiments with a mildly better conditioned problem\n(\u03ba(J) = 100 ,\u03ba(H) = 10 ) and a variety of batch sizes, with results in Figure 4. The results show that\nSPRING consistently provides acceleration over SNGD, with the amount of acceleration shrinking\nfor the largest batch sizes, as expected. Regarding SGD, we observe that it converges as fast as\nSNGD when k= 1 and significantly slower in every other case, with the gap becoming enormous\nfor large batch sizes. Interestingly SNGD exhibits some instabilities when k= 1, perhaps because\nthe hyperparameter search makes an aggressive choice of the step size.\n0 2000 4000 6000 8000 10000\nIteration100101Solution error ||*||\nk=1\nSGD\nSNGD\nSPRING\n0 2000 4000 6000 8000 10000\nIteration105\n104\n103\n102\n101\n100101Solution error ||*||\nk=3\nSGD\nSNGD\nSPRING\n0 2000 4000 6000 8000 10000\nIteration1012\n1010\n108\n106\n104\n102\n100Solution error ||*||\nk=10\nSGD\nSNGD\nSPRING\n0 250 500 750 1000 1250 1500 1750 2000\nIteration1012\n1010\n108\n106\n104\n102\n100Solution error ||*||\nk=30\nSGD\nSNGD\nSPRING\nFigure 4: SGD, SNGD, and SPRING for a randomly generated instance of Problem 1 with varying\nbatch sizes. All methods have their hyperparameters tuned independently for each problem. Note\nthat the x-axis is shortened in the bottom right plot to account for the faster convergence of SNGD\nandSPRING when k= 30 .\n1\nDISCLAIMER\nThis report was prepared as an account of work sponsored by an agency of the United States Gov-\nernment. Neither the United States Government nor any agency thereof, nor any of their employees,\nmakes any warranty, express or implied, or assumes any legal liability or responsibility for the ac-\ncuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or\nrepresents that its use would not infringe privately owned rights. Reference herein to any specific\ncommercial product, process, or service by trade name, trademark, manufacturer, or otherwise does\nnot necessarily constitute or imply its endorsement, recommendation, or favoring by the United\nStates Government or any agency thereof. The views and opinions of authors expressed herein do\nnot necessarily state or reflect those of the United States Government or any agency thereof.\n21",
  "project_dir": "artifacts/projects/enhanced_stat.ML_2508.21022v1_Fast_Convergence_Rates_for_Subsampled_Natural_Grad",
  "communication_dir": "artifacts/projects/enhanced_stat.ML_2508.21022v1_Fast_Convergence_Rates_for_Subsampled_Natural_Grad/.agent_comm",
  "assigned_at": "2025-08-29T20:58:13.529911",
  "status": "assigned"
}